{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import seaborn as sns\n",
    "import seaborn.objects as so\n",
    "\n",
    "import os\n",
    "import math\n",
    "\n",
    "# Seaborn theme\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Functions\n",
    "def safe_read_csv(file_path):\n",
    "    try:\n",
    "        return pd.read_csv(file_path)\n",
    "    except Exception:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data config and such\n",
    "bucket_numbers = [1, 16, 32, 64, 128, 256]\n",
    "\n",
    "# Load all data into memory\n",
    "dump_folder = \"dumps/\"\n",
    "\n",
    "app_names_read = sorted([app_name for app_name in os.listdir(dump_folder)])\n",
    "app_names = [s.replace('_', ' ') for s in app_names_read]\n",
    "\n",
    "stats_dfs = list()\n",
    "tags_dfs = list()\n",
    "recvs_dfs = list()\n",
    "\n",
    "for app in app_names_read:\n",
    "    local_stats_dfs = list()\n",
    "    local_tags_dfs = list()\n",
    "    local_recvs_dfs = list()\n",
    "    for bucket_size in bucket_numbers:\n",
    "        local_stats_dfs.append(safe_read_csv(os.path.join(dump_folder, app, \"df{}\".format(bucket_size), \"stats.csv\")))\n",
    "        local_tags_dfs.append(safe_read_csv(os.path.join(dump_folder, app, \"df{}\".format(bucket_size), \"tags.csv\")))\n",
    "        local_recvs_dfs.append(safe_read_csv(os.path.join(dump_folder, app, \"df{}\".format(bucket_size), \"recvs.csv\")))\n",
    "\n",
    "    stats_dfs.append(local_stats_dfs)\n",
    "    tags_dfs.append(local_tags_dfs)\n",
    "    recvs_dfs.append(local_recvs_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relative percentage of p2p vs collectives vs rdma\n",
    "\n",
    "instructions_res = []\n",
    "for (app_name, stat_dfs) in zip(app_names, stats_dfs):   \n",
    "    # Skip empty applications\n",
    "    if (len(stat_dfs) != len(bucket_numbers)) or stat_dfs[0] is None:\n",
    "        continue\n",
    "\n",
    "    stat_df = stat_dfs[0]\n",
    "\n",
    "    # total_rdma = stat_df['number_rdma'].sum()\n",
    "    total_p2p = stat_df['number_p2p'].sum()\n",
    "    total_collectives = stat_df['number_collectives'].sum()\n",
    "    # total_inst = total_rdma + total_p2p + total_collectives\n",
    "    total_inst = total_p2p + total_collectives\n",
    "\n",
    "    # row_val = (app_name,  total_p2p / total_inst * 100, total_collectives / total_inst * 100, total_rdma / total_inst * 100)\n",
    "    row_val = (app_name,  total_p2p / total_inst * 100, total_collectives / total_inst * 100)\n",
    "    instructions_res.append(row_val)\n",
    "\n",
    "# instructions_df = pd.DataFrame(instructions_res, columns=['Application', 'Point-to-point', 'Collectives', 'One-sided'])\n",
    "instructions_df = pd.DataFrame(instructions_res, columns=['Application', 'Point-to-point', 'Collectives'])\n",
    "\n",
    "ax = instructions_df.plot(kind='bar', stacked=True, x='Application')\n",
    "plt.ylabel('Distribution of MPI calls')\n",
    "plt.legend(loc='lower center')\n",
    "ax.yaxis.set_major_formatter(ticker.PercentFormatter())\n",
    "\n",
    "plt.savefig(\"pictures/app-ops-analysis.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of tags used (number unique tags vs application number)\n",
    "# Should I use a scatter plot?\n",
    "\n",
    "unique_tags_per_app = dict()\n",
    "for (app_name, tag_dfs) in zip(app_names, tags_dfs): \n",
    "    # Skip empty applications\n",
    "    if (len(tag_dfs) != len(bucket_numbers)) or tag_dfs[0] is None:\n",
    "        continue\n",
    "\n",
    "    tag_df = tag_dfs[0]\n",
    "    unique = len(tag_df.columns) - 3 # Columns are: rank, op, kind, tag#1, tag#2, ..., tag#n\n",
    "    \n",
    "    # Skip collective-only applications\n",
    "    if unique == 0:\n",
    "        continue\n",
    "\n",
    "    if unique not in unique_tags_per_app:\n",
    "        unique_tags_per_app[unique] = 0 # Init value    \n",
    "\n",
    "    unique_tags_per_app[unique] += 1\n",
    "\n",
    "condensed_unique_tags = [0,0,0,0,0,0] # Data is splitted by 1 tag, 2 tags, 3 tags and 4+, 100+, 1000+ tags\n",
    "for key in unique_tags_per_app:\n",
    "    insert_in = 0\n",
    "    if key >= 1000:\n",
    "        insert_in = 5\n",
    "    elif key >= 100:\n",
    "        insert_in = 4\n",
    "    elif key >= 10:\n",
    "        insert_in = 3\n",
    "    else:\n",
    "        insert_in = key - 1\n",
    "\n",
    "    condensed_unique_tags[insert_in] += unique_tags_per_app[key]\n",
    "\n",
    "# Calculate aprox. 50% of applications\n",
    "total_apps = len(app_names)\n",
    "acc = 0\n",
    "for (pos, unique_tags) in enumerate(condensed_unique_tags):\n",
    "     acc += unique_tags\n",
    "     if (acc >= math.floor(total_apps * 0.5)):\n",
    "          plt.axvline(pos - 0.5, color='red')\n",
    "          print(acc)\n",
    "          break\n",
    "\n",
    "tag_usage_df = dict(zip(['1', '2', '3', '10+', '100+', '1000+'], condensed_unique_tags))\n",
    "\n",
    "ax = sns.barplot(tag_usage_df)\n",
    "plt.xlabel('Number of unique tags used')\n",
    "plt.ylabel('Number of applications')\n",
    "\n",
    "for axis in [ax.xaxis, ax.yaxis]:\n",
    "            axis.set_major_locator(ticker.MaxNLocator(integer=True))\n",
    "\n",
    "# plt.savefig(\"pictures/app-tag-usage.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of unique pair source-tag used vs application number\n",
    "# Should I use a scatter plot?\n",
    "\n",
    "combinations = pd.DataFrame(columns=['app', 'unique_pairs', 'total_pairs'])\n",
    "for (app_name, recv_dfs) in zip(app_names, recvs_dfs): \n",
    "    # Skip empty applications\n",
    "    if (len(recv_dfs) != len(bucket_numbers)) or recv_dfs[0] is None:\n",
    "        continue\n",
    "\n",
    "    recv_df = recv_dfs[0]\n",
    "\n",
    "    uniques = len(recv_df[['src', 'tag']].drop_duplicates())\n",
    "    total = len(recv_df)\n",
    "\n",
    "    if total == 0:\n",
    "        continue\n",
    "\n",
    "    combinations.loc[len(combinations)] = [app_name, uniques, total]\n",
    "\n",
    "combinations['perc'] = combinations['unique_pairs'] / combinations['total_pairs'] * 100\n",
    "\n",
    "# ax = sns.barplot(combinations.sort_values('perc', ascending=False), y='perc', x='app') # sort by %\n",
    "ax = sns.barplot(combinations, y='perc', x='app') # no sort. app alphabetically\n",
    "ax.tick_params(axis='x', rotation=90)\n",
    "ax.yaxis.set_major_formatter(ticker.PercentFormatter(decimals=None))\n",
    "\n",
    "ax.set_xlabel(\"Application\")\n",
    "ax.set_ylabel(\"Unique combinations of source and tag in receives\")\n",
    "\n",
    "plt.savefig(\"pictures/app-unique-tags.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ranks per application\n",
    "for (app_name, stat_dfs) in zip(app_names, stats_dfs):\n",
    "    # Skip empty applications\n",
    "    if (len(stat_dfs) != len(bucket_numbers)) or stat_dfs[0] is None:\n",
    "        continue\n",
    "\n",
    "    stat_df = stat_dfs[0]\n",
    "    ranks = stat_df['rank'].max() + 1\n",
    "    \n",
    "    print(\"{:<40} = {:<4} processes\".format(app_name, ranks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tags used per application\n",
    "for (app_name, tag_dfs) in zip(app_names, tags_dfs): \n",
    "    # Skip empty applications\n",
    "    if (len(tag_dfs) != len(bucket_numbers)) or tag_dfs[0] is None:\n",
    "        continue\n",
    "\n",
    "    tag_df = tag_dfs[0]\n",
    "    unique = len(tag_df.columns) - 3 # Columns are: rank, op, kind, tag#1, tag#2, ..., tag#n\n",
    "    \n",
    "    print(\"{:<40} = {:<4} unique tags\".format(app_name, unique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Presence of wildcards per app\n",
    "for (app_name, stat_dfs) in zip(app_names, stats_dfs):\n",
    "    # Skip empty applications\n",
    "    if (len(stat_dfs) != len(bucket_numbers)) or stat_dfs[0] is None:\n",
    "        continue\n",
    "\n",
    "    stat_df = stat_dfs[0]\n",
    "\n",
    "    src_wildcards = stat_df[['recv_src_wildcard_sync', 'recv_src_wildcard_async']].agg({\n",
    "        'recv_src_wildcard_sync': 'sum',\n",
    "        'recv_src_wildcard_async': 'sum',\n",
    "    }).sum()\n",
    "\n",
    "    tag_wildcards = stat_df[['recv_tag_wildcard_sync', 'recv_tag_wildcard_async']].agg({\n",
    "        'recv_tag_wildcard_sync': 'sum',\n",
    "        'recv_tag_wildcard_async': 'sum',\n",
    "    }).sum()\n",
    "\n",
    "    double_wildcards = stat_df[['recv_double_wildcard_sync', 'recv_double_wildcard_async']].agg({\n",
    "        'recv_double_wildcard_sync': 'sum',\n",
    "        'recv_double_wildcard_async': 'sum',\n",
    "    }).sum()\n",
    "\n",
    "    print(\"{:<40} src = {:<6} tag = {:<6} double = {:<6}\".format(app_name, src_wildcards, tag_wildcards, double_wildcards))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timeline of max. queue depth for each app\n",
    "# Ranks per app are grouped and the max (change operation to check) value is taken\n",
    "\n",
    "for (app_name, stat_dfs) in zip(app_names, stats_dfs):   \n",
    "    # Skip empty applications\n",
    "    if (len(stat_dfs) != len(bucket_numbers)) or stat_dfs[0] is None:\n",
    "        continue\n",
    "\n",
    "    # Top ylim for bottom ax comparing 1 bin vs 16 bins\n",
    "    ylim_start = (stat_dfs[1])['collisions_no_wildcard_max_len'].max()\n",
    "\n",
    "    # More precise to which plots are halfs and which aren't. Shows less lines on top half\n",
    "    ylim_end = (stat_dfs[0])['collisions_no_wildcard_max_len'].max() - abs(pow((stat_dfs[0])['collisions_no_wildcard_max_len'].mean(), 0.75))\n",
    "    # Shows the same number of lines on each half\n",
    "    # ylim_end = (stat_dfs[0])['collisions_no_wildcard_max_len'].max() - ylim_start\n",
    "\n",
    "    dist = abs(ylim_start - ylim_end)\n",
    "\n",
    "    # print(f\"dist = {dist} vs ylim_start = {ylim_start} | {app_name}\")\n",
    "\n",
    "    plot_df = pd.DataFrame()\n",
    "    for (bucket_size, stat_df) in zip(bucket_numbers, stat_dfs):\n",
    "        # local_df = stat_df[['rank', 'collisions_no_wildcard_max_len', 'collisions_src_wildcard_max_len', \n",
    "        #                     'collisions_tag_wildcard_max_len', 'collisions_double_wildcard_max_len']].copy()\n",
    "                \n",
    "        local_df = stat_df[['rank', 'collisions_no_wildcard_max_len']].copy()\n",
    "\n",
    "        local_df['group'] = local_df.groupby('rank').cumcount()\n",
    "        local_df = local_df.groupby('group').agg({\n",
    "            'collisions_no_wildcard_max_len': 'max'\n",
    "        }).reset_index(drop=True)     \n",
    "\n",
    "        plot_df[str(bucket_size)] = local_df.copy()\n",
    "    \n",
    "    if dist < ylim_start:\n",
    "        fig, ax = plt.subplots()\n",
    "\n",
    "        sns.lineplot(plot_df, ax=ax)\n",
    "\n",
    "        ax.set_title(f\"Timeline of max. queue depth for {app_name}\")\n",
    "        ax.set_xlabel(\"Runtime\")\n",
    "        ax.set_ylabel(\"Max. Queue depth\", loc='bottom')\n",
    "        ax.legend(title=\"Total bins\")\n",
    "\n",
    "        ax.yaxis.set_major_locator(plt.MaxNLocator(integer=True))\n",
    "\n",
    "        ax.set_xticks([])\n",
    "    else:\n",
    "        fig, (ax_top, ax_bottom) = plt.subplots(nrows=2, sharex=True, gridspec_kw={'hspace':0.05})\n",
    "\n",
    "        d = .015\n",
    "        kwargs = dict(transform=ax_top.transAxes, color='k', clip_on=False)\n",
    "        ax_top.plot((-d, +d), (-d, +d), **kwargs)\n",
    "        kwargs.update(transform=ax_bottom.transAxes)\n",
    "        ax_bottom.plot((-d, +d), (1 - d, 1 + d), **kwargs)\n",
    "\n",
    "        sns.lineplot(plot_df, ax=ax_top)\n",
    "        sns.lineplot(plot_df, ax=ax_bottom)\n",
    "        \n",
    "        ax_top.set_ylim(bottom=ylim_end)\n",
    "        ax_bottom.set_ylim(0, ylim_start + 1.0)\n",
    "\n",
    "        ax_bottom.legend_.remove()\n",
    "\n",
    "        ax_top.set_title(f\"Timeline of max. queue depth for {app_name}\")\n",
    "        ax_bottom.set_xlabel(\"Runtime\")\n",
    "        ax_top.set_ylabel(\"Max. Queue depth\", loc='bottom')\n",
    "        ax_top.legend(title=\"Total bins\")\n",
    "    \n",
    "        ax_top.yaxis.set_major_locator(plt.MaxNLocator(integer=True))\n",
    "        ax_bottom.yaxis.set_major_locator(plt.MaxNLocator(integer=True))\n",
    "\n",
    "        ax_bottom.set_xticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timeline of %occupancy bins\n",
    "# Ranks per app and grouped and the min (change operation to check) value is taken\n",
    "\n",
    "for (app_name, stat_dfs) in zip(app_names, stats_dfs):   \n",
    "    # Skip empty applications\n",
    "    if (len(stat_dfs) != len(bucket_numbers)) or stat_dfs[0] is None:\n",
    "        continue\n",
    "\n",
    "    plot_df = pd.DataFrame()\n",
    "    for (bucket_size, stat_df) in zip(bucket_numbers, stat_dfs):\n",
    "        # local_df = stat_df[['rank', 'collisions_no_wildcard_max_len', 'collisions_src_wildcard_max_len', \n",
    "        #                     'collisions_tag_wildcard_max_len', 'collisions_double_wildcard_max_len']].copy()\n",
    "                \n",
    "        local_df = stat_df[['rank', 'empty_bins_perc_no_wildcard']].copy()\n",
    "\n",
    "        local_df['group'] = local_df.groupby('rank').cumcount()\n",
    "        local_df = local_df.groupby('group').agg({\n",
    "            'empty_bins_perc_no_wildcard': 'min'\n",
    "        }).reset_index(drop=True)     \n",
    "\n",
    "        plot_df[str(bucket_size)] = 1.0 - local_df.copy()\n",
    "   \n",
    "    plot_df = plot_df * 100\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.lineplot(plot_df, ax=ax)\n",
    "    \n",
    "    plt.title(f\"Timeline of % occupied bins for {app_name}\")\n",
    "    plt.xlabel(\"Runtime\")\n",
    "    plt.ylabel(\"Occupied bins\")\n",
    "    plt.legend(title=\"Total bins\")\n",
    "    \n",
    "    ax.yaxis.set_major_formatter(ticker.PercentFormatter(decimals=None))\n",
    "    ax.set_xticks([])\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timeline of max. collisions for each app\n",
    "# Ranks per app are grouped and the max (change operation to check) value is taken\n",
    "\n",
    "for (app_name, stat_dfs) in zip(app_names, stats_dfs):   \n",
    "    # Skip empty applications\n",
    "    if (len(stat_dfs) != len(bucket_numbers)) or stat_dfs[0] is None:\n",
    "        continue\n",
    "\n",
    "    # Top ylim for bottom ax comparing 1 bin vs 16 bins\n",
    "    ylim_start = (stat_dfs[1])['collisions_no_wildcard'].max()\n",
    "\n",
    "    # More precise to which plots are halfs and which aren't. Shows less lines on top half\n",
    "    ylim_end = (stat_dfs[0])['collisions_no_wildcard'].max() - abs(pow((stat_dfs[0])['collisions_no_wildcard'].mean(), 0.75))\n",
    "    # Shows the same number of lines on each half\n",
    "    # ylim_end = (stat_dfs[0])['collisions_no_wildcard_max_len'].max() - ylim_start\n",
    "\n",
    "    dist = abs(ylim_start - ylim_end)\n",
    "\n",
    "    # print(f\"dist = {dist} vs ylim_start = {ylim_start} | {app_name}\")\n",
    "\n",
    "    plot_df = pd.DataFrame()\n",
    "    for (bucket_size, stat_df) in zip(bucket_numbers, stat_dfs):\n",
    "        # local_df = stat_df[['rank', 'collisions_no_wildcard_max_len', 'collisions_src_wildcard_max_len', \n",
    "        #                     'collisions_tag_wildcard_max_len', 'collisions_double_wildcard_max_len']].copy()\n",
    "                \n",
    "        local_df = stat_df[['rank', 'collisions_no_wildcard']].copy()\n",
    "\n",
    "        local_df['group'] = local_df.groupby('rank').cumcount()\n",
    "        local_df = local_df.groupby('group').agg({\n",
    "            'collisions_no_wildcard': 'max'\n",
    "        }).reset_index(drop=True)     \n",
    "\n",
    "        plot_df[str(bucket_size)] = local_df.copy()\n",
    "    \n",
    "    if dist < ylim_start:\n",
    "        fig, ax = plt.subplots()\n",
    "\n",
    "        sns.lineplot(plot_df, ax=ax)\n",
    "\n",
    "        ax.set_title(f\"Timeline of max. collisions for {app_name}\")\n",
    "        ax.set_xlabel(\"Runtime\")\n",
    "        ax.set_ylabel(\"Max. Collisions\", loc='bottom')\n",
    "        ax.legend(title=\"Total bins\")\n",
    "\n",
    "        ax.yaxis.set_major_locator(plt.MaxNLocator(integer=True))\n",
    "\n",
    "        ax.set_xticks([])\n",
    "    else:\n",
    "        fig, (ax_top, ax_bottom) = plt.subplots(nrows=2, sharex=True, gridspec_kw={'hspace':0.05})\n",
    "\n",
    "        d = .015\n",
    "        kwargs = dict(transform=ax_top.transAxes, color='k', clip_on=False)\n",
    "        ax_top.plot((-d, +d), (-d, +d), **kwargs)\n",
    "        kwargs.update(transform=ax_bottom.transAxes)\n",
    "        ax_bottom.plot((-d, +d), (1 - d, 1 + d), **kwargs)\n",
    "\n",
    "        sns.lineplot(plot_df, ax=ax_top)\n",
    "        sns.lineplot(plot_df, ax=ax_bottom)\n",
    "        \n",
    "        ax_top.set_ylim(bottom=ylim_end)\n",
    "        ax_bottom.set_ylim(0, ylim_start + 1.0)\n",
    "\n",
    "        ax_bottom.legend_.remove()\n",
    "\n",
    "        ax_top.set_title(f\"Timeline of max. queue depth for {app_name}\")\n",
    "        ax_bottom.set_xlabel(\"Runtime\")\n",
    "        ax_top.set_ylabel(\"Max. Queue depth\", loc='bottom')\n",
    "        ax_top.legend(title=\"Total bins\")\n",
    "    \n",
    "        ax_top.yaxis.set_major_locator(plt.MaxNLocator(integer=True))\n",
    "        ax_bottom.yaxis.set_major_locator(plt.MaxNLocator(integer=True))\n",
    "\n",
    "        ax_bottom.set_xticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timeline of collisions for each app\n",
    "# Ranks per app and grouped and the max (change operation to check) value is taken\n",
    "\n",
    "for (app_name, stat_dfs) in zip(app_names, stats_dfs):   \n",
    "    # Skip empty applications\n",
    "    if (len(stat_dfs) != len(bucket_numbers)) or stat_dfs[0] is None:\n",
    "        continue\n",
    "    \n",
    "    max_rank = stat_dfs[0]['rank'].max() + 1\n",
    "\n",
    "    # Limit of ranks to plot. Applications with more than the limit are skipped (too much clutter)\n",
    "    if max_rank > 8:\n",
    "        continue\n",
    "\n",
    "    num_rows = (max_rank + 4 - 1) // 4\n",
    "\n",
    "    fig, ax = plt.subplots(nrows = num_rows, ncols = 4, sharex=True, sharey=True, figsize=(10,2))\n",
    "\n",
    "    for rank in range(max_rank):\n",
    "        plot_df = pd.DataFrame()        \n",
    "        for (bucket_size, stat_df) in zip(bucket_numbers, stat_dfs):\n",
    "            selection_df = stat_df.loc[stat_df['rank'] == rank]\n",
    "            selection_df = selection_df['collisions_no_wildcard_max_len'].reset_index(drop=True)  \n",
    "\n",
    "            plot_df[str(bucket_size)] = selection_df.copy()\n",
    "\n",
    "        selected = plt.subplot(num_rows, 4, rank + 1)\n",
    "        selected.plot(plot_df)\n",
    "        selected.yaxis.set_major_locator(plt.MaxNLocator(integer=True))\n",
    "        selected.set_xticks([])\n",
    "    \n",
    "    fig.legend(bucket_numbers, title='Bucket Size')\n",
    "    fig.text(0.5, 0.04, 'Runtime', ha='center', va='center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sumarize apps using configured bins. Get max and average and plot of bins usage\n",
    "# Plotted using 1, 32 and 128 bins\n",
    "\n",
    "bins = [1, 32, 128]\n",
    "bins_pos = [0, 2, 4]\n",
    "\n",
    "plot_df = pd.DataFrame(columns=['app','Num. Bins', 'max', 'avg'])\n",
    "avgs = list()\n",
    "for (bin_pos, bin) in zip(bins_pos, bins):\n",
    "    bin_avgs = list()\n",
    "    max_avgs = list()\n",
    "    for (app_name, stat_dfs) in zip(app_names, stats_dfs):\n",
    "        # Skip empty applications\n",
    "        if (len(stat_dfs) != len(bucket_numbers)) or stat_dfs[0] is None:\n",
    "            continue\n",
    "\n",
    "        # Bucket selected\n",
    "        stat_df = stat_dfs[bin_pos]\n",
    "\n",
    "        local_df = stat_df[['rank', 'empty_bins_perc_no_wildcard']].copy()\n",
    "\n",
    "        my_max = (1.0 - local_df['empty_bins_perc_no_wildcard'].min()) * 100\n",
    "    \n",
    "        local_df['group'] = local_df.groupby('rank').cumcount()\n",
    "        local_df = local_df.groupby('group').agg({\n",
    "            'empty_bins_perc_no_wildcard': 'mean'\n",
    "        }).reset_index(drop=True)\n",
    "\n",
    "        my_mean = (1.0 - local_df['empty_bins_perc_no_wildcard'].min()) * 100\n",
    "        \n",
    "        bin_avgs.append(my_mean)\n",
    "        max_avgs.append(my_max)\n",
    "\n",
    "        plot_df.loc[len(plot_df)] = [app_name, bin, my_max, my_mean]\n",
    "    \n",
    "    avgs.append((pd.DataFrame(bin_avgs)[0].mean(), pd.DataFrame(max_avgs)[0].mean()))\n",
    "\n",
    "\n",
    "\n",
    "plot_df = plot_df.sort_values(by=['Num. Bins', 'avg'], ascending=[True, False])\n",
    "\n",
    "# plot_df\n",
    "\n",
    "grid = sns.FacetGrid(plot_df, col='Num. Bins', sharey=True, sharex=False)\n",
    "grid.map(sns.barplot, 'app', 'avg', label='Avg.')\n",
    "grid.map(sns.scatterplot, 'app', 'max', label='Max.', color='orange')\n",
    "grid.set_axis_labels('Application', 'Used bins')\n",
    "\n",
    "grid.add_legend(title='Statistic')\n",
    "\n",
    "for (ax, (avg, max)) in zip(grid.axes.flat, avgs):\n",
    "    ax.axhline(avg, ls='--', color='r')\n",
    "    # ax.axhline(max, ls='--', color='g')\n",
    "    ax.tick_params(axis='x', rotation=90)\n",
    "    ax.yaxis.set_major_formatter(ticker.PercentFormatter(decimals=None))\n",
    "\n",
    "# Ignore order warning, input is sorted beforehand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sumarize apps using configured bins. Get max and average and plot of queue depth\n",
    "# Plotted using 1, 32 and 128 bins\n",
    "\n",
    "bins = [1, 32, 128]\n",
    "bins_pos = [0, 2, 4]\n",
    "\n",
    "plot_df = pd.DataFrame(columns=['app','Num. Bins', 'max', 'avg'])\n",
    "avgs = list()\n",
    "for (bin_pos, bin) in zip(bins_pos, bins):\n",
    "    bin_avgs = list()\n",
    "    max_avgs = list()\n",
    "    for (app_name, stat_dfs) in zip(app_names, stats_dfs):\n",
    "        # Skip empty applications\n",
    "        if (len(stat_dfs) != len(bucket_numbers)) or stat_dfs[0] is None:\n",
    "            continue\n",
    "\n",
    "        # Bucket selected\n",
    "        stat_df = stat_dfs[bin_pos]\n",
    "\n",
    "        local_df = stat_df[['rank', 'collisions_no_wildcard_max_len']].copy()\n",
    "\n",
    "        my_max = local_df['collisions_no_wildcard_max_len'].max()\n",
    "\n",
    "        local_df['group'] = local_df.groupby('rank').cumcount()\n",
    "        local_df = local_df.groupby('group').agg({\n",
    "            'collisions_no_wildcard_max_len': 'mean'\n",
    "        }).reset_index(drop=True)\n",
    "\n",
    "\n",
    "        my_mean = local_df['collisions_no_wildcard_max_len'].max()\n",
    "        \n",
    "        bin_avgs.append(my_mean)\n",
    "        max_avgs.append(my_max)\n",
    "\n",
    "        plot_df.loc[len(plot_df)] = [app_name, bin, my_max, my_mean]\n",
    "    \n",
    "    avgs.append((pd.DataFrame(bin_avgs)[0].mean(), pd.DataFrame(max_avgs)[0].mean()))\n",
    "\n",
    "\n",
    "\n",
    "plot_df = plot_df.sort_values(by=['Num. Bins', 'avg'], ascending=[True, False])\n",
    "\n",
    "# plot_df\n",
    "\n",
    "grid = sns.FacetGrid(plot_df, col='Num. Bins', sharey=False, sharex=False)\n",
    "grid.map(sns.barplot, 'app', 'avg', label='Avg.')\n",
    "grid.map(sns.scatterplot, 'app', 'max', label='Max.', color='orange')\n",
    "grid.set_axis_labels('Application', 'Queue depth [number of elements]')\n",
    "\n",
    "grid.add_legend(title='Statistic')\n",
    "\n",
    "for (ax, (avg, max)) in zip(grid.axes.flat, avgs):\n",
    "    ax.axhline(avg, ls='--', color='r')\n",
    "    # ax.axhline(max, ls='--', color='g')\n",
    "    ax.tick_params(axis='x', rotation=90)\n",
    "\n",
    "# Ignore order warning, input is sorted beforehand\n",
    "\n",
    "plt.savefig(\"pictures/q-depth.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sumarize apps using configured bins. Get max and average and plot of collisions\n",
    "# Plotted using 1, 32 and 128 bins\n",
    "\n",
    "bins = [1, 32, 128]\n",
    "bins_pos = [0, 2, 4]\n",
    "\n",
    "plot_df = pd.DataFrame(columns=['app','Num. Bins', 'max', 'avg'])\n",
    "avgs = list()\n",
    "for (bin_pos, bin) in zip(bins_pos, bins):\n",
    "    bin_avgs = list()\n",
    "    max_avgs = list()\n",
    "    for (app_name, stat_dfs) in zip(app_names, stats_dfs):\n",
    "        # Skip empty applications\n",
    "        if (len(stat_dfs) != len(bucket_numbers)) or stat_dfs[0] is None:\n",
    "            continue\n",
    "\n",
    "        # Bucket selected\n",
    "        stat_df = stat_dfs[bin_pos]\n",
    "\n",
    "        local_df = stat_df[['rank', 'collisions_no_wildcard']].copy()\n",
    "\n",
    "        my_max = local_df['collisions_no_wildcard'].max()\n",
    "\n",
    "        local_df['group'] = local_df.groupby('rank').cumcount()\n",
    "        local_df = local_df.groupby('group').agg({\n",
    "            'collisions_no_wildcard': 'mean'\n",
    "        }).reset_index(drop=True)\n",
    "\n",
    "\n",
    "        my_mean = local_df['collisions_no_wildcard'].max()\n",
    "        \n",
    "        bin_avgs.append(my_mean)\n",
    "        max_avgs.append(my_max)\n",
    "\n",
    "        plot_df.loc[len(plot_df)] = [app_name, bin, my_max, my_mean]\n",
    "    \n",
    "    avgs.append((pd.DataFrame(bin_avgs)[0].mean(), pd.DataFrame(max_avgs)[0].mean()))\n",
    "\n",
    "\n",
    "\n",
    "plot_df = plot_df.sort_values(by=['Num. Bins', 'avg'], ascending=[True, False])\n",
    "\n",
    "# plot_df\n",
    "\n",
    "grid = sns.FacetGrid(plot_df, col='Num. Bins', sharey=False, sharex=False)\n",
    "grid.map(sns.barplot, 'app', 'avg', label='Avg.')\n",
    "grid.map(sns.scatterplot, 'app', 'max', label='Max.', color='orange')\n",
    "grid.set_axis_labels('Application', 'Number of collisions')\n",
    "\n",
    "grid.add_legend(title='Statistic')\n",
    "\n",
    "for (ax, (avg, max)) in zip(grid.axes.flat, avgs):\n",
    "    ax.axhline(avg, ls='--', color='r')\n",
    "    # ax.axhline(max, ls='--', color='g')\n",
    "    ax.tick_params(axis='x', rotation=90)\n",
    "\n",
    "# Ignore order warning, input is sorted beforehand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimissing returns for increase in bins. ahhaha dont shows want I want\n",
    "\n",
    "plot_df = pd.DataFrame(columns=['bin', 'avg'])\n",
    "\n",
    "for (bin, bin_idx) in zip(bucket_numbers, range(0, len(bucket_numbers))):\n",
    "    bin_avg_list = list()\n",
    "    for (app_name, stat_dfs) in zip(app_names, stats_dfs):\n",
    "        # Skip empty applications\n",
    "        if (len(stat_dfs) != len(bucket_numbers)) or stat_dfs[0] is None:\n",
    "            break\n",
    "\n",
    "        local_df = stat_dfs[bin_idx][['rank', 'collisions_no_wildcard_max_len']].copy()\n",
    "        \n",
    "        local_df['group'] = local_df.groupby('rank').cumcount()\n",
    "        local_df = local_df.groupby('group').agg({\n",
    "            'collisions_no_wildcard_max_len': 'mean'\n",
    "        }).reset_index(drop=True)\n",
    "\n",
    "        my_mean = local_df['collisions_no_wildcard_max_len'].max()\n",
    "        bin_avg_list.append(my_mean)\n",
    "    \n",
    "    plot_df.loc[len(plot_df)] = [str(bin), pd.DataFrame(bin_avg_list)[0].mean()]\n",
    "\n",
    "plot_df['improvement'] = plot_df.iloc[0,1] / plot_df['avg']\n",
    "plot_df['perc'] = (plot_df.iloc[0,1] - plot_df['avg']) / plot_df.iloc[0,1]\n",
    "\n",
    "print(plot_df.iloc[0,1])\n",
    "\n",
    "# plot_df = plot_df[:-1]\n",
    "\n",
    "# plot_df['improvement'] = plot_df['avg'].div(plot_df['avg'].shift(1))\n",
    "# plot_df = plot_df[1:]\n",
    "\n",
    "ax = sns.lineplot(plot_df, x='bin', y='perc')\n",
    "# ax.set_ylim([0.0, 30.0])\n",
    "\n",
    "plot_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mpi-trace-sim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
